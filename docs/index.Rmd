---
title: "<img src=\"bellabeat logo.png\" height=\"35\" style=\"float: left; padding: 0px 15px 0px 0px;\"/> Bellabeat Case Study"
toc-title: "Table of Contents"
output:
  html_document:
    toc: true
    theme: paper
---

```{css, echo = FALSE}
h2 {
  color: #F68D77;
}
```
```{css, echo = FALSE}
h1 {
  display: inline-block;
  font-weight: bold;
  color: #F68D77;
}
```

## 1. Summary

Bellabeat is a high-tech company that manufactures health-focused smart products. Collecting data on activity, sleep, stress, and reproductive health has allowed Bellabeat to empower women with knowledge about their own health habits.

The purpose of this study is to analyze non-Bellabeat smart device usage in order to gain insight into how consumers are already using their devices. For this case, we will apply our insights to the Bellabeat app, with possible applications for the Bellabeat Time watch.

## 2. Ask Questions

### 2.1 Business Task

As mentioned above, the task at hand is to analyze smart device usage data in order to gain insights into how consumers use non-Bellabeat smart devices. We will then apply the following insights to one Bellabeat product:

1.  What are some trends in smart device usage?
2.  How could these trends apply to Bellabeat customers?
3.  How could these trends help influence Bellabeat marketing strategy?

## 3. Gather + Validate Data

### 3.1 Dataset Used

The data sourced is Fitbit Fitness Tracker Data (CC0: Public Domain; made available through Mobius) via Kaggle.

This Kaggle dataset contains personal fitness tracker data from 33 Fitbit users. These datasets were generated by respondents to a distributed survey via Amazon Mechanical Turk between 03.12.2016-05.12.2016. The 33 eligible Fitbit users consented to the submission of personal tracker data, including minute-level output for physical activity, heart rate, and sleep monitoring. Variation between output represents use of different types of Fitbit trackers and individual tracking behaviors/preferences.

### 3.2 Accessability and Privacy Concerns

This data is confirmed to be open-source, and open access on OpenAIRE index. It's licensed under a CC0: Public Domain as well.

All eligible participants who qualified for the study, based on their answers, completed an electronic informed consent and became part of the Mturk Prime panel. Researchers involved in the panel never had face-to-face contact with particpants to ensure privacy.

### 3.3 Bias and Credibility of Dataset

**Potential issue with bias:** Amazon Mturk survey participants received monetary compensation in exchange for non-identifiable data to be collected. In addition, this study required respondents to have access to a Fitbit device in order to participate. Individuals who can afford and use Fitbit devices and other consumer wearables are more likely to be younger (between the ages of 18 and 34) and affluent, thus impacting generalizability.

### 3.4 Data Organization and Selection

The dataset provided includes 18 .csv files. Most of the files are in long format, although three are in wide. Each of the files contains information regarding participants' sleep, METs, calories, steps and other common fitness metrics. Each participant is identified via a unique user Id.

For the sake of this study, not all of the files provided will be used for the following reasons:

-   They are subsets of larger or more relevant formats (i.e. 'Seconds'/'Minutes' data will be foregone in exchange for 'Hourly'/'Daily' data; "dailySteps" information is housed within "dailyActivity")

-   The sample size is too small

### 3.5 Potential Limitations

-   **Missing Metadata and Demographics:** There is no information regarding the age, gender, location, or fitness level of each participant. Not having this information makes it harder to compare metrics across demographics.

-   **Small Sample Size:** With only 33 users, it's harder to draw widely-applicable conclusions due to the limited pool of eligible participants.

-   **Short Collection Period:** The data collected was over the course of 31 days. Had there been a longer data collection period, the data available would've been more representative of habits over time.

## 4. Clean + Pre-Process Data

### 4.1 Install Packages and Load Libraries

We will primarily use the "tidyverse", "forcats", and "RColorBrewer" packages, so those need to be installed prior to loading libraries. 

```{r, message=FALSE}
install.packages("tidyverse")
install.packages("forcats")
install.packages("RColorBrewer")
```


Then the following libraries will be loaded:

-   tidyverse
-   lubridate
-   dplyr
-   ggplot2
-   forcats
-   RColorBrewer

```{r, message=FALSE}
library(tidyverse)
library(lubridate)
library(dplyr)
library(ggplot2)
library(forcats)
library(RColorBrewer)
```

### 4.2 Import Data

In order to best serve our task at hand, we will import only the most relevant data:

-   dailyActivity
-   hourlySteps
-   sleepDay

```{r, message=FALSE}
daily_activity <- read_csv("dailyActivity_merged.csv")
hourly_steps <- read_csv("hourlySteps_merged.csv")
sleep <- read_csv("sleepDay_merged.csv")
```

### 4.3 Preview Data

In order to clean and process the files we imported, we need to see what variables and data types are housed within each imported data frame:

```{r}
head(daily_activity, n=3)
head(hourly_steps, n=3)
head(sleep, n=3)
```

We can see that we're working with many variables such as steps, calories, distance, active minutes, and minutes asleep. In order to move forward, we'll have to filter out the most pertinent information that we have.

### 4.4 Clean Data

Now that we know what kind of data we have at hand, we can begin the process of cleaning each dataset.

#### 4.41 Verifying number of participants

We need to make sure we have 33 unique Ids (the sleep data has only 24 participants, but we'll keep it for any further sleep analysis).

```{r, results='hide'}
n_distinct(daily_activity$Id)
n_distinct(hourly_steps$Id)
n_distinct(sleep$Id)
```

#### 4.42 Looking for duplicates

Duplicates will throw off our analysis, so let's see if there are any.

```{r}
sum(duplicated(daily_activity))
sum(duplicated(hourly_steps))
sum(duplicated(sleep))
```

#### 4.43 Removing duplicates from 'sleep'

As there were 3 duplicates in 'sleep', we need to drop 'em.

```{r, results='hide'}
sleep <- sleep %>% distinct()
```

Confirmation of dropped duplicates?

```{r}
sum(duplicated(sleep))
```

Confirmed.

#### 4.44 Removing N/As or NULLs

Like with the duplicates, N/As or NULL values do us no good. So let's drop those too.

```{r, results='hide'}
daily_activity %>% drop_na()
hourly_steps %>% drop_na()
sleep %>% drop_na()
```

Let's verify the N/As have been dropped.

```{r}
sum(is.na(daily_activity))
sum(is.na(hourly_steps))
sum(is.na(sleep))
```

Fantastic. On to formatting and merging data frames.

### 4.5 Formatting and Merging

Now we need to make our data frames compatible with each other and easily referenced for future analysis.

#### 4.51 Making date columns consistent across all data frames

We can't hope to merge or compare a key variable if the columns are named inconsistently.

```{r, message=FALSE}
names(daily_activity)[2] <- "Date"
names(hourly_steps)[2] <- "DateTime"
names(sleep)[2] <- "Date"
```

#### 4.52 Formatting date columns

If our date column doesn't register as 'datetime' format (dttm), it will be hard to parse out the hourly column for our analysis later.

```{r, message=FALSE}
daily_activity$Date <- as.POSIXct(daily_activity$Date, format = "%m/%d/%Y", tz = Sys.timezone())
hourly_steps$DateTime <- as.POSIXct(hourly_steps$DateTime, format = "%m/%d/%Y %I:%M:%S %p", tz = Sys.timezone())
sleep$Date <- as.POSIXct(sleep$Date, format = "%m/%d/%Y", tz = Sys.timezone())
```

Let's confirm the columns are in datetime (dttm) format.

```{r}
head(daily_activity, n=3)
head(hourly_steps, n=3)
head(sleep, n=3)
```

Perfect. Now let's separate the Date and Time in our 'hourly steps' table as mentioned before. That way we can isolate the hour or date of activity as needed.

```{r, message=FALSE}
hourly_steps <- hourly_steps %>% 
  mutate(DateTime = parse_date_time(DateTime, "Y-m-d H:M:S")) %>%
  mutate(Date = as.Date(DateTime), Hour = format(DateTime, format = "%H:%M:%S"))
```

Now we need to drop the "DateTime" column since we now have separate "Date" and "Hour" columns to work with. Then we'll reorder the columns so they're closer to the "Id".

```{r, message=FALSE}
hourly_steps <- hourly_steps[-2]
```

```{r, message=FALSE}
hourly_steps <- hourly_steps %>% relocate(Date, .after=Id) %>% relocate(Hour, .after=Date)
```

#### 4.53 Adding "Weekday" and "TotalActiveMinutes" columns

Now we'll add "Weekday" columns to translate the dates into days of the week.

```{r, message=FALSE}
hourly_steps$Weekday <- wday(hourly_steps$Date, label=TRUE)
daily_activity$Weekday <- wday(daily_activity$Date, label=TRUE)
sleep$Weekday <- wday(sleep$Date, label=TRUE)
```

Then we'll reorder the columns to move "Weekday" closer to "Date"

```{r, message=FALSE}
hourly_steps <- hourly_steps %>% relocate(Weekday, .after=Date)
daily_activity <- daily_activity %>% relocate(Weekday, .after=Date)
sleep <- sleep %>% relocate(Weekday, .after=Date)
```

Now we'll add a "TotalActiveMinutes" column to add up all the active minutes collected per participant each day. We'll then reorder the columns so it's not at the very end.

```{r, message=FALSE}
daily_activity <- daily_activity %>% mutate(TotalActiveMinutes = VeryActiveMinutes+FairlyActiveMinutes+LightlyActiveMinutes)
daily_activity <- daily_activity %>% relocate(TotalActiveMinutes, .before=Calories)
```

#### 4.54 Merging 'sleep' and 'daily_activity' into one table

So we don't have to bounce back and forth between the sleep and daily activity tables, we'll merge them. For any analysis that requires sleep data we'll reference this table. For analysis that doesn't include sleep, we'll stick to the original 'daily_activity' table, as that includes all 33 participants (whereas the new merged table will only have 24).

```{r,message=FALSE}
daily_sleep_activity <- merge(sleep, daily_activity, by = c("Id", "Date", "Weekday"))
```

## 5. Analyze + Visualize Data

### 5.1 Steps, Sleep, and Total Active Minutes

To kick off our analysis, we need to see what trends are present between total minutes asleep, total steps, and total active minutes.

#### 5.11 Compiling averages by day of the week

Let's see what the average total minutes asleep, total steps, and total active minutes are per weekday.

```{r, message=FALSE}
weekday_averages <- daily_sleep_activity %>% 
  group_by(Weekday) %>% 
  summarize(daily_steps = mean(TotalSteps), daily_sleep = mean(TotalMinutesAsleep), daily_active_mins = mean(TotalActiveMinutes))
```

```{r}
head(weekday_averages, 7)
```

From this table we can see that:

-   Saturday is when users have the highest average step total; Sunday is the lowest

-   Sunday is best for minutes asleep; Thursday is the worst

-   Saturday has the highest total active minutes; Sunday has the lowest

Now let's check out the correlations between the three variables above.

```{r}
cor(daily_sleep_activity$TotalSteps, daily_sleep_activity$TotalMinutesAsleep)
cor(daily_sleep_activity$TotalActiveMinutes, daily_sleep_activity$TotalMinutesAsleep)
cor(daily_sleep_activity$TotalActiveMinutes, daily_sleep_activity$TotalSteps)
```

It looks like there isn't much of a correlation (positive or negative) between sleep and total active minutes nor total steps. However, total active minutes and total steps are strongly correlated, which makes sense.

For the sake of our analysis - and giving as much insight as we can - we'll focus on total steps as our activity metric since there is both hourly and daily data available for total steps. So let's look more into our total average steps.

#### 5.12 Finding peak days and hours for average total steps

We know that Saturdays have the highest average step totals. But what times? And are there other days/times when step total averages are high?

```{r, message=FALSE}
hourly_steps %>% group_by(Weekday, Hour) %>% summarise(avg_steps = mean(StepTotal)) %>% arrange(desc(avg_steps))
```

It looks like Saturdays 11a-2p, Wednesdays 5-6p, and Mondays 6-7p are when average step totals are their highest across all participants. This might be helpful to know later on, so let's turn this data into its own data frame.

```{r, message=FALSE}
popular_step_times <- hourly_steps %>% 
  group_by(Weekday, Hour) %>% 
  summarise(avg_steps = mean(StepTotal)) %>%
  arrange(desc(avg_steps))
```

Let's plot the general trends of average step totals across the entire week. While the micro data of knowing [what specific days and hours](https://public.tableau.com/app/profile/shea3379/viz/BellabeatAverageHourlyStepsbyUserID/Sheet1#1) average step totals are highest, it's also helpful to see a more macro view of the whole week.

```{r}
popular_step_times %>% 
  group_by(Hour) %>% 
  summarize(avgsteptime = mean(avg_steps)) %>% 
  ggplot() +
  geom_col(mapping = aes(x=Hour, y = avgsteptime, fill = avgsteptime)) + 
  labs(title = "Average Hourly Steps", x="", y="") + 
  labs(subtitle = "As an average across all days of the week") +
  labs(fill = "Average Steps") +
  scale_fill_gradientn(colors = c("#FBD2CA", "#f68d77", "#34073d")) +
  theme(axis.text.x = element_text(angle = 90))
```

Now we have an overview of which times are most active for steps across any day of the week. No surprise that lunchtime (12-2p) and after work (5-7p) have the highest step average.

### 5.2 Participant Activity Levels

We know that there are two common metrics to determine the activity level of participants:

-   Steps per day (goal: 10,000 steps)

-   Weekly minutes of moderate exercise (goal: 150+ minutes)

Let's see what our participants' activity levels are for both.

#### 5.21 Activity level as steps per day

First, we need to see what each participant's average daily step total is.

```{r, message=FALSE}
user_activity_levels <- daily_activity %>% 
  group_by(Id) %>% 
  summarize(Avg_Daily_Steps = mean(TotalSteps))
```

Now, we need to classify them by [levels of activity](https://www.medicinenet.com/how_many_steps_a_day_is_considered_active/article.htm) according to the commonly accepted "10,000 steps per day" guidelines.

```{r, message=FALSE}
user_activity_levels <- user_activity_levels %>% 
  mutate(User_Level = case_when(
    Avg_Daily_Steps < 5000 ~ "Sedentary",
    Avg_Daily_Steps >= 5000 & Avg_Daily_Steps <7499 ~ "Low Active",
    Avg_Daily_Steps >= 7500 & Avg_Daily_Steps <9999 ~ "Somewhat Active",
    Avg_Daily_Steps >= 10000 & Avg_Daily_Steps < 12499 ~ "Active",
    Avg_Daily_Steps >= 12500 ~ "Very Active"))
```

```{r}
head(user_activity_levels)
```

And let's add a column to signify whether or not each participant's activity level met or exceeded the 10,000 step benchmark.

```{r, message=FALSE}
user_activity_levels <- user_activity_levels %>% 
  mutate(Met_Goal = case_when(
    Avg_Daily_Steps >= 0 & Avg_Daily_Steps <10000 ~ "No",
    Avg_Daily_Steps >= 10000 ~ "Yes"
  ))
```

```{r}
head(user_activity_levels)
```

Great. Now let's look at our activity level data as a percentage of all participants. What percentage of our participant pool makes up each activity level? And what percentage of our participant pool meets or exceeds the 10,000 steps per day guideline?

```{r, message=FALSE}
percent_user_type <- user_activity_levels %>% 
  group_by(User_Level) %>% 
  summarize(Total = n()) %>% 
  mutate(Percent_of_Users = (Total/33)*100) 
```

```{r}
head(percent_user_type)
```

Now let's reorder the activity level categories from Very Active to Sedentary for the sake of plotting later.

```{r, message=FALSE}
percent_user_type$User_Level <- factor(percent_user_type$User_Level, levels = c("Very Active", "Active", "Somewhat Active", "Low Active", "Sedentary"))
```

Now let's plot our breakdown of activity levels for all of our participants.

```{r}
ggplot(percent_user_type, aes(fill=User_Level, y=Total, x=factor(1))) +
  geom_bar(position="fill", stat="identity") +
  geom_text(aes(label=paste0(sprintf("%1.1f%%", Percent_of_Users))),
            position=position_fill(vjust=0.5), colour="white", size =4) +
  labs(title = "Activity Level Based on Daily Step Total") +
  labs(subtitle = "Percentage of all participants that fall into each activity level") +
  labs(x = "Activity Level", y = "Percentage", fill = "Activity Levels") +
  scale_fill_discrete(labels = c("Very Active (>12500 steps)", "Active (10000-12499 steps)", "Somewhat Active (7500-9999 steps)", "Low Active (5000-7499 steps)", "Sedentary (<5000 steps)"))
```

We can see that a majority of our participants take less than 10,000 steps per day on average. So let's create a specific data frame and plot to represent that.

```{r, message=FALSE}
percent_goal_met <- user_activity_levels %>% 
  group_by(Met_Goal) %>% 
  summarize(Total = n()) %>% 
  mutate(Percent_of_Users = (Total/33)*100)
```

```{r}
head(percent_goal_met)
```

And let's plot it out.

```{r, message=FALSE}
ggplot(percent_goal_met, aes(fill=fct_reorder(Met_Goal, Total), y=Total, x=factor(1))) +
  geom_bar(position="fill", stat="identity") +
  geom_text(aes(label=paste0(sprintf("%1.1f%%", Percent_of_Users))),
            position=position_fill(vjust=0.5), colour="white", size =4) +
  labs(title = "CDC Step Goal (10,000+ Steps)") +
  labs(subtitle = "Percentage of total participants whose average step total met the 10,000 steps/day benchmark") +
  labs(x = "Goal Met", y = "Total Percentage", fill = "Goal Met?") +
  scale_fill_discrete(labels = c("Yes (10,000+ steps)", "No (<10,000 steps)")) +
  scale_fill_manual(values = c("#00BF7D", "#F8766D"))
```

As we can see, **almost 79% of our participants** don't meet the 10,000 steps per day goal. This data could prove useful in terms of opportunity to motivate and market to consumers. But we're not done yet. Let's see if our participants meet the recommended 150+ minutes of moderate (Fairly Active and Very Active) activity per week.

#### 5.22 Activity level as weekly minutes of moderate+ activity

First, we need to add a column calculating the total number of moderate or higher activity minutes per user, per day (as a sum of FairlyActive and VeryActiveMinutes).

```{r, message=FALSE}
daily_activity <- daily_activity %>% 
  mutate(ModeratePlus_Activity = VeryActiveMinutes+FairlyActiveMinutes)
```

Now we need to find the average of moderate+ active mins per participant for each weekday.

```{r, message=FALSE}
most_active_days <- daily_activity %>% 
  group_by(Id, Weekday) %>% 
  summarise(avg_activemins = mean(ModeratePlus_Activity)) %>%
  arrange(Id)
```

From here we can consolidate the weekday data into *weekly* data. This will give us the average weekly minutes of moderate+ activity for each participant.

```{r, message=FALSE}
weekly_active_mins <- most_active_days %>% 
  group_by(Id) %>% 
  summarize(weekly_activeminutes = sum(avg_activemins)) %>% 
  arrange(Id)
```

```{r}
head(weekly_active_mins)
```

Finally, let's add a column that will signify whether or not each participant met the goal of 150+ minutes of moderate+ activity each week.

```{r, message=FALSE}
weekly_active_mins <- weekly_active_mins %>% 
  mutate(Met_Goal = case_when(
    weekly_activeminutes >= 0 & weekly_activeminutes <150 ~ "No",
    weekly_activeminutes >= 150 ~ "Yes"
  ))
```

```{r}
head(weekly_active_mins)
```

Awesome. Now we're going to calculate what percentage of our participant pool met the goal of 150+ minutes of moderate+ activity each week (on average).

```{r, message=FALSE}
modactivitygoal_met <- weekly_active_mins %>% 
  group_by(Met_Goal) %>% 
  summarize(Total = n()) %>% 
  mutate(Percent_of_Users = (Total/33)*100)
```

```{r}
head(modactivitygoal_met)
```

Now let's plot it for maximum visual effect.

```{r}
ggplot(modactivitygoal_met, aes(fill=fct_reorder(Met_Goal, -Total), y=Total, x=factor(1))) +
  geom_bar(position="fill", stat="identity") +
  geom_text(aes(label=paste0(sprintf("%1.1f%%", Percent_of_Users))),
            position=position_fill(vjust=0.5), colour="white", size =4) +
  labs(title = "CDC Weekly Moderate Activity Goal (150+ Minutes)") +
  labs(subtitle = "Percentage of participants who met the goal of 150+ minutes of moderate activity per week") +
  labs(x = "Goal Met", y = "Total Percentage", fill = "Goal Met?") +
  scale_fill_discrete(labels = c("Yes (150+ minutes)", "No (<150 minutes)")) +
  scale_fill_manual(values = c("#00BF7D", "#F8766D"))
```

Here we can see that **almost half** of our participants met the goal of 150+ minutes of moderate or higher activity. Again, this is a great opportunity to motivate users who may not be reaching this recommended goal.

But in order to do that, it would be great if we could see how our participants fare over the course of an average week. What days of the week are usually lower in term of moderate activity? What days of the week are higher?

If we can establish a daily baseline of 22 minutes each day (150 minutes/7 days a week), we can track when during the week our participants are starting to fall behind. So let's do that.

#### 5.23 Activity level as daily minutes of moderate+ activity

First we'll qualify our 'most_active_days' data to see if each user met the goal of 22 average daily minutes of moderate+ activity.

```{r, message=FALSE}
most_active_days <- most_active_days %>% 
  mutate(Met_Goal = case_when(
    avg_activemins >= 0 & avg_activemins <22 ~ "No",
    avg_activemins >=22 ~ "Yes"
  ))
```

```{r}
head(most_active_days)
```

Now, using a table we'll count how many participants - on average - met the 22 minute daily goal for each weekday.

```{r}
table(most_active_days$Weekday, most_active_days$Met_Goal)
```

Then we'll save it as a value for reference later.

```{r, message=FALSE}
modactivitytable=table(most_active_days$Weekday, most_active_days$Met_Goal)
```

Now let's turn that "table-turned-value" into a workable data frame.

```{r, message=FALSE}
dailymodgoal_met <- as.data.frame.matrix(modactivitytable)
dailymodgoal_met$Weekday <- rownames(modactivitytable)
row.names(dailymodgoal_met) <- c(1, 2, 3, 4, 5, 6, 7)
```

```{r}
head(dailymodgoal_met)
```

From here, let's turn the number of participants who did (Yes) and didn't (No) meet the daily goal into percentages.

```{r, message=FALSE}
dailymodgoal_met <- dailymodgoal_met %>%
  group_by(Weekday) %>% 
  mutate(YesPercent = (Yes/(Yes+No)*100)) %>% 
  mutate(NoPercent = (No/(Yes+No)*100))
```

```{r}
head(dailymodgoal_met)
```

Awesome. Now we know what percentage of our total participants had a daily average that met the 22+ minutes of moderate+ activity.

In order to plot this cleanly, we're going to reshape the data. First, let's transform the 'dailymodgoal_met' data frame into a new data frame with just the percentages represented.

```{r, message=FALSE}
dailymodgoal_met2 <- data.frame(Day = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"),
                             Yes = c(50.0, 59.4, 48.5, 54.5, 45.5, 45.5, 50.0),
                             No = c(50.0, 40.6, 51.5, 45.5, 54.5, 54.5, 50.0))
```

```{r}
head(dailymodgoal_met2)
```

Next, let's transform this new data frame into long format so the data points we need to plot will be more easily referenced.

```{r, message=FALSE}
dailymodgoal_metTall <- dailymodgoal_met2 %>% 
  gather(key = GoalMet, value = Value, Yes:No)
```

```{r}
head(dailymodgoal_metTall)
```

Looks good. Now we'll arrange our days of the week in order (again, to make our lives easier for plotting).

```{r, message=FALSE}
dailymodgoal_metTall$Day <- factor(dailymodgoal_metTall$Day, levels = c('Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'))
```

```{r}
head(dailymodgoal_metTall)
```

Finally, we can plot our data.

```{r}
ggplot(dailymodgoal_metTall, aes(x=Day, y=Value, fill = fct_rev(GoalMet))) +
  geom_col(width = 0.5, position = "dodge") +
  expand_limits(y=c(0, 100)) +
  labs(title = "Hitting the Daily Moderate Activity Goal") +
  labs(subtitle = "Percentage of participants who completed 22+ minutes of moderate or higher activity") +
  labs(x = "Weekday", y = "Percent", fill = "Goal Met?") +
  scale_fill_manual(values = c("#00BF7D", "#F8766D"))
```

As we can see, there isn't a large difference between participants who hit their daily moderate activity goal and those who didn't. But on Thursdays and Fridays, participants were more likely to **not** hit the 22 minute benchmark. Monday, however, is the strongest day for participants to exceed their activity benchmark.

If we wanted to break this data down further and [filter by participant](https://public.tableau.com/app/profile/shea3379/viz/BellabeatCaseStudyDailyModerateActivity/AverageModerateActiveMinutes#1), we would find a more individualized result. Some participants are generally more active than others, so being able to filter this kind of data by participant would allow us to consider consumers on a case-by-case basis.

### 5.3 Device Usage per Participant

No matter what activity trends we uncover, if our consumers aren't tapped into their smart devices on a regular basis, our data will be useless. So let's make sure our potential consumers are inclined to use their devices (at least most of the time).

First, let's group our participants into usage categories based on the number of days they provided data.

```{r, message=FALSE}
daily_usage <- daily_activity %>% 
  group_by(Id) %>%
  summarize(days_active = sum(n())) %>% 
  mutate(usage_level = case_when(
    days_active >= 1 & days_active <= 10 ~ "Low Use",
    days_active >= 11 & days_active <= 20 ~ "Moderate Use",
    days_active >= 21 & days_active <= 31 ~ "High Use",
  ))
```

```{r}
head(daily_usage)
```

Next, let's calculate the percentage of participants who fall into each usage category.

```{r, message=FALSE}
percent_usage_levels <- daily_usage %>% 
  group_by(usage_level) %>% 
  summarize(Total = n_distinct(Id)) %>% 
  mutate(Percent_of_Users = (Total/33)*100)
```

```{r}
head(percent_usage_levels)
```

Now we're ready to plot the results.

```{r}
ggplot(percent_usage_levels, aes(fill=fct_reorder(usage_level, Percent_of_Users), y=Total, x=factor(1))) +
  geom_bar(position="fill", stat="identity") +
  geom_text(aes(label=paste0(sprintf("%1.1f%%", Percent_of_Users))),
            position=position_fill(vjust=0.5), colour="white", size =4) +
  labs(title = "Device Usage Across All Participants") +
  labs(subtitle = "Comparing what percentage of partcipants are high, moderate, and low users") +
  labs(x = "Usage Level", y = "Total Percentage", fill = "Usage Levels") +
  scale_fill_discrete(labels = c("Low Use (0-10 days)", "Moderate Use (11-20 days)", "High Use (21-31 days)")) +
  scale_fill_manual(values = c("#F8766D","#619CFF", "#00BF7D"))
```

It turns out that **nearly 88% of participants** use their device most of the time (21 days or more out of 31 days available). If this data serves as an indicator, we can confidently use our device data to form recommendations. Speaking of which, let's wrap this analysis up with some recommended actions for Bellabeat.

## 6. Recommended Actions

In order for Bellabeat to properly cater and market to their preferred audience, it would be wise to take this data with a grain of salt. Due to the limitations regarding missing demographics, small sample size, and limited data collection window, it's hard to form comprehensive suggestions.

However, using the data we have, we suggest:

-   **Personalized Activity Markers**

    Each Bellabeat user will be different. Knowing the metrics of each user on a daily (and hourly) basis will help Bellabeat provide personalized, timely recommendations. If a user isn't hitting their daily exercise goals, Bellabeat can send an alert right before their most active hour of activity.

    Our step data is a perfect example of this. We saw which days and hours [were most active for each participant](https://public.tableau.com/app/profile/shea3379/viz/BellabeatAverageHourlyStepsbyUserID/Sheet1#1) from our hourly step data. If Bellabeat wants to encourage more steps or minutes out of their users using the principles of inertia (an object in motion, stays in motion), they know exactly when to do it: right before their most active hours.

    Using the data they have on the activity habits of their users to encourage more activity from them will put Bellabeat at a competitive advantage. Users will be launching themselves into more activity without having to summon up the willpower at random times each day.

-   **'Time' Watch Interface Updates**

    As we saw, nearly 88% of participants were high usage participants. However, according to the Bellabeat, their Time watch doesn't receive notifications like their app does.

    In order to effectively alert users while they're active, the Time watch will need to be able to receive notifications on its own. That way, the watch can not only track hourly data, but it can turn that data into a timely notification directly to the user without them having to locate their phone.

Bellabeat has a unique marketing advantage if the above recommendations are implemented. Not only will they be using their hourly data to predict when their users are most active, they'll harness that information to encourage their users to stay active the minute they start.

Inertia and motivation [can be powerful](https://ap-lab.ca/wp-content/uploads/2020/06/Camera_Ready___On_the_Impact_of_Context_Aware_Notifications_on_Exercising_compressed.pdf) when used together. People are more likely to stay active once they begin if they receive a well-timed encouragement from their wearable. And Bellabeat can be at the forefront of this shift in smart device integration.

#### Special Thanks

I'd like to offer gratitude to my fellow Kagglers who helped me form my analysis. Their work paved the way for me to thoroughly explore this dataset with confidence (and saved me time by providing pertinent functions).

Thank you, [Zulkhairee Zulaiman](https://www.kaggle.com/code/zulkhaireesulaiman/bellabeat-capstone-project-in-r) and [Macarena Lacasa](https://www.kaggle.com/code/macarenalacasa/capstone-case-study-bellabeat)
